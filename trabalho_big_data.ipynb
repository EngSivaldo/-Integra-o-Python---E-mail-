{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngSivaldo/-Integra-o-Python---E-mail-/blob/main/trabalho_big_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raa8pHuHdkZj"
      },
      "source": [
        "# Comandos para realizaÃ§Ã£o do trabalho da matÃ©ria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "Este notebook foi projetado para guiar os alunos na realizaÃ§Ã£o das prÃ¡ticas de Big Data utilizando PySpark. Certifique-se de seguir cada etapa cuidadosamente para garantir a correta execuÃ§Ã£o das atividades.\n",
        "\n",
        "Seu trabalho comeÃ§arÃ¡ na cÃ©lula 5. Execute as 4 primeiras cÃ©lulas para iniciar a atividade.\n",
        "\n",
        "## <font color=red>ObservaÃ§Ã£o importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas nÃ£o serÃ¡ aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcvGRW8kdkZl"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab\n",
        "\n",
        "Aqui, vocÃª farÃ¡ o download do dataset necessÃ¡rio para as atividades. Certifique-se de que o arquivo foi descompactado corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "e54FoMBzdkZm",
        "outputId": "ceedae82-00ec-4e16-e693-1adda70cb665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-18 23:09:59--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: â€˜imdb-reviews-pt-br.zipâ€™\n",
            "\n",
            "\rimdb-reviews-pt-br.   0%[                    ]       0  --.-KB/s               \rimdb-reviews-pt-br. 100%[===================>]  47.25M   246MB/s    in 0.2s    \n",
            "\n",
            "2025-05-18 23:10:00 (246 MB/s) - â€˜imdb-reviews-pt-br.zipâ€™ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "replace imdb-reviews-pt-br.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-qJVr2dkZn"
      },
      "source": [
        "## InstalaÃ§Ã£o manual das dependÃªncias para uso do pyspark no Google Colab\n",
        "\n",
        "Esta etapa garante que todas as bibliotecas necessÃ¡rias para o PySpark sejam instaladas no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COjZ7agkdkZn"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN4_Wh_DdkZo"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession\n",
        "\n",
        "A SparkSession Ã© o ponto de entrada para usar o PySpark. Certifique-se de configurar corretamente o nome do aplicativo e o master."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_jQ0zaPzdkZo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atPpgw2vdkZp"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o mÃ©todo read.csv do spark\n",
        "\n",
        "NÃ£o altere este cÃ³digo e use o dataframe imdb_df criado aqui em todo o seu trabalho. A criaÃ§Ã£o de um dataframe diferente deste poderÃ¡ causar erros na coluna sentiment e isso refletirÃ¡ em erros de resposta das questÃµes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-nr5sWPPdkZp"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeqjIczDdkZp"
      },
      "source": [
        "# QuestÃ£o 1\n",
        "\n",
        "Nesta questÃ£o, vocÃª irÃ¡ calcular a soma dos IDs para entradas onde o sentimento ('sentiment') Ã© 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhC3ICjcdkZq"
      },
      "source": [
        "## Criar funÃ§Ãµes de MAP:\n",
        "- Criar funÃ§Ã£o para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A funÃ§Ã£o map irÃ¡ transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "A0tFLvK9dkZq",
        "outputId": "ae86e81c-4316-46a0-b567-212a25da5d5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“¦ INICIANDO ANÃLISE DA ESTRUTURA DO DATAFRAME...\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "ğŸ” COLUNAS DISPONÃVEIS E TIPOS DE DADOS NO DATAFRAME:\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- text_en: string (nullable = true)\n",
            " |-- text_pt: string (nullable = true)\n",
            " |-- sentiment: string (nullable = true)\n",
            "\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "ğŸ“Œ INTERPRETAÃ‡ÃƒO:\n",
            "  â€¢ 'id'         â†’ identificador do registro (string)\n",
            "  â€¢ 'text_en'    â†’ texto da review em inglÃªs\n",
            "  â€¢ 'text_pt'    â†’ texto da review em portuguÃªs\n",
            "  â€¢ 'sentiment'  â†’ sentimento da review (pos ou neg)\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "ğŸ“¦ Dados processados com sucesso!\n",
            "\n",
            "ğŸ” Tipo de anÃ¡lise: Soma dos IDs para sentimentos negativos\n",
            "ğŸ§® MÃ©todo SQL â†’ Resultado: 459568555\n",
            "ğŸ§® MÃ©todo RDD (map/reduce) â†’ Resultado: 459568555\n",
            "\n",
            "ğŸ“ˆ ConclusÃ£o: Ambos os mÃ©todos retornaram o mesmo valor.\n",
            "âœ… A soma foi validada com sucesso!\n",
            "\n",
            "ğŸ“ InformaÃ§Ãµes do participante:\n",
            "   - Nome: Sivaldo\n",
            "   - Registro UniversitÃ¡rio (RU): 4139872\n",
            "\n",
            "ğŸ“Š Resultado Final:\n",
            "   A soma total dos IDs associados ao sentimento 'neg' Ã© **459,568,555**.\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Œ InformaÃ§Ãµes do aluno\n",
        "ru = \"4139872\"\n",
        "nome = \"Sivaldo\"\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# ğŸ“¦ INICIANDO ANÃLISE DA ESTRUTURA DO DATAFRAME...\n",
        "print(\"\\nğŸ“¦ INICIANDO ANÃLISE DA ESTRUTURA DO DATAFRAME...\\n\")\n",
        "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
        "print(\"ğŸ” COLUNAS DISPONÃVEIS E TIPOS DE DADOS NO DATAFRAME:\")\n",
        "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\")\n",
        "\n",
        "# Exibir o schema do DataFrame\n",
        "imdb_df.printSchema()\n",
        "\n",
        "print(\"\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
        "print(\"ğŸ“Œ INTERPRETAÃ‡ÃƒO:\")\n",
        "print(\"  â€¢ 'id'         â†’ identificador do registro (string)\")\n",
        "print(\"  â€¢ 'text_en'    â†’ texto da review em inglÃªs\")\n",
        "print(\"  â€¢ 'text_pt'    â†’ texto da review em portuguÃªs\")\n",
        "print(\"  â€¢ 'sentiment'  â†’ sentimento da review (pos ou neg)\")\n",
        "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\")\n",
        "\n",
        "# ğŸ“Œ Removendo registros com 'id' nulo ou vazio\n",
        "df_valid = imdb_df.filter((col(\"id\").isNotNull()) & (col(\"id\") != \"\"))\n",
        "\n",
        "# ğŸ” Convertendo coluna 'id' para inteiro\n",
        "df_valid = df_valid.withColumn(\"id_int\", col(\"id\").cast(IntegerType()))\n",
        "\n",
        "# ğŸ” Filtrando apenas sentimentos negativos com IDs vÃ¡lidos\n",
        "df_neg = df_valid.filter((col(\"sentiment\") == \"neg\") & col(\"id_int\").isNotNull())\n",
        "\n",
        "# âœ… Soma com Spark SQL (forma segura)\n",
        "soma_sql = df_neg.agg({\"id_int\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "# FunÃ§Ã£o MAP para o RDD\n",
        "def map1(row):\n",
        "    try:\n",
        "        return (row[\"sentiment\"], int(row[\"id\"]))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# FunÃ§Ã£o REDUCE para o RDD\n",
        "def reduceByKey1(x, y):\n",
        "    return x + y\n",
        "\n",
        "# Aplicando map/reduce com RDD\n",
        "resultado_rdd = imdb_df.rdd.map(map1).filter(lambda x: x is not None and x[0] == \"neg\").reduceByKey(reduceByKey1).collect()\n",
        "soma_rdd = [v for k, v in resultado_rdd if k == \"neg\"][0]\n",
        "\n",
        "# ğŸ¯ ExibiÃ§Ã£o final com layout profissional\n",
        "print(\"ğŸ“¦ Dados processados com sucesso!\\n\")\n",
        "print(\"ğŸ” Tipo de anÃ¡lise: Soma dos IDs para sentimentos negativos\")\n",
        "print(f\"ğŸ§® MÃ©todo SQL â†’ Resultado: {soma_sql}\")\n",
        "print(f\"ğŸ§® MÃ©todo RDD (map/reduce) â†’ Resultado: {soma_rdd}\")\n",
        "\n",
        "if soma_sql == soma_rdd:\n",
        "    print(\"\\nğŸ“ˆ ConclusÃ£o: Ambos os mÃ©todos retornaram o mesmo valor.\")\n",
        "    print(\"âœ… A soma foi validada com sucesso!\\n\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ ATENÃ‡ÃƒO: As somas nÃ£o coincidem. Verifique os dados!\")\n",
        "\n",
        "print(\"ğŸ“ InformaÃ§Ãµes do participante:\")\n",
        "print(f\"   - Nome: {nome}\")\n",
        "print(f\"   - Registro UniversitÃ¡rio (RU): {ru}\")\n",
        "\n",
        "print(\"\\nğŸ“Š Resultado Final:\")\n",
        "print(f\"   A soma total dos IDs associados ao sentimento 'neg' Ã© **{soma_sql:,.0f}**.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOCzLDcndkZq"
      },
      "source": [
        "## Cria funÃ§Ãµes de REDUCE:\n",
        "\n",
        "- Criar funÃ§Ã£o de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A funÃ§Ã£o reduce irÃ¡ somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKZuQd2IdkZq"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x,y):\n",
        "  # Coloque aqui o seu cÃ³digo para retornar o resultado necessÃ¡rio.\n",
        "  # x e y sÃ£o valores que serÃ£o somados, pois o reduceByKey receberÃ¡\n",
        "  # apenas o segundo elemento da tupla vinda da saÃ­da da funÃ§Ã£o map.\n",
        "  # Apague a linha abaixo para iniciar seu cÃ³digo.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5_3nbKRdkZr"
      },
      "source": [
        "## AplicaÃ§Ã£o do map/reduce e visualizaÃ§Ã£o do resultado\n",
        "\n",
        "Aqui, vocÃª aplicarÃ¡ as funÃ§Ãµes de map e reduce ao dataframe Spark para calcular os resultados. NÃ£o se esqueÃ§a de usar o mÃ©todo `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BanrXF13dkZr"
      },
      "outputs": [],
      "source": [
        "# Linha de cÃ³digo para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.rdd.map(map1).reduceByKey(reduceByKey1).collect()\n",
        "# Coloque aqui o cÃ³digo para imprimir o resultado. NÃ£o esqueÃ§a seu RU:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHJT9-AzdkZr"
      },
      "source": [
        "# QuestÃ£o 2:\n",
        "\n",
        "Nesta questÃ£o, vocÃª irÃ¡ calcular a diferenÃ§a no nÃºmero total de palavras entre textos negativos em portuguÃªs e inglÃªs.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') Ã© 'neg'.\n",
        "- Subtrair o total de palavras em inglÃªs do total em portuguÃªs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFO4r3PSdkZr"
      },
      "source": [
        "## Criar funÃ§Ãµes de MAP:\n",
        "- Criar funÃ§Ã£o para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A funÃ§Ã£o map irÃ¡ transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para entÃ£o descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o mÃ©todo .split() e nÃ£o .split(\" \") de string para separar as palavras em uma lista ou use a funÃ§Ã£o split(coluna de texto, regex) do pyspark com o regex igual Ã  \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "eLzfbFGBdkZs",
        "outputId": "c48a2d98-092b-4dcb-9cc9-533429937248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Iniciando processamento dos dados...\n",
            "ğŸ” Contando palavras nos textos negativos (inglÃªs e portuguÃªs)...\n",
            "ğŸ” Aplicando map/reduce para validaÃ§Ã£o...\n",
            "\n",
            "ğŸ“¦ Dados processados com sucesso!\n",
            "\n",
            "ğŸ” Tipo de anÃ¡lise: DiferenÃ§a no total de palavras em textos negativos\n",
            "ğŸ§® Total palavras (InglÃªs) â†’ 5,400,297\n",
            "ğŸ§® Total palavras (PortuguÃªs) â†’ 5,455,273\n",
            "ğŸ§® DiferenÃ§a (PT - EN) â†’ 54,976\n",
            "\n",
            "ğŸ“ˆ ConclusÃ£o: âœ… VALIDAÃ‡ÃƒO BEM-SUCEDIDA! Contagens coincidem.\n",
            "\n",
            "ğŸ“ InformaÃ§Ãµes do participante:\n",
            "   - Nome: Sivaldo\n",
            "   - Registro UniversitÃ¡rio (RU): 4139872\n",
            "\n",
            "ğŸ“Š Resultado Final:\n",
            "   Os textos negativos em portuguÃªs possuem **54,976** palavras a mais do que os textos em inglÃªs.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ImportaÃ§Ãµes necessÃ¡rias\n",
        "from pyspark.sql.functions import col, split, size\n",
        "\n",
        "# Dados do participante\n",
        "ru = \"4139872\"\n",
        "nome = \"Sivaldo\"\n",
        "\n",
        "print(\"ğŸ“¦ Iniciando processamento dos dados...\")\n",
        "\n",
        "# Filtrar registros com sentimento negativo\n",
        "neg_df = imdb_df.filter(col(\"sentiment\") == \"neg\")\n",
        "\n",
        "print(\"ğŸ” Contando palavras nos textos negativos (inglÃªs e portuguÃªs)...\")\n",
        "\n",
        "# Contar palavras na coluna 'text_en'\n",
        "neg_df = neg_df.withColumn(\"count_en\", size(split(col(\"text_en\"), r\"\\s+\")))\n",
        "\n",
        "# Contar palavras na coluna 'text_pt'\n",
        "neg_df = neg_df.withColumn(\"count_pt\", size(split(col(\"text_pt\"), r\"\\s+\")))\n",
        "\n",
        "# Somar o total de palavras por idioma\n",
        "soma_en = neg_df.agg({\"count_en\": \"sum\"}).collect()[0][0]\n",
        "soma_pt = neg_df.agg({\"count_pt\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "# Calcular diferenÃ§a total (pt - en)\n",
        "diferenca = soma_pt - soma_en\n",
        "\n",
        "print(\"ğŸ” Aplicando map/reduce para validaÃ§Ã£o...\")\n",
        "\n",
        "# FunÃ§Ã£o map para mapear sentimento e contagem de palavras\n",
        "def map2(row):\n",
        "    return (row['sentiment'], (row['count_en'], row['count_pt']))\n",
        "\n",
        "# FunÃ§Ã£o reduce para somar palavras por idioma\n",
        "def reduceByKey2(x, y):\n",
        "    return (x[0] + y[0], x[1] + y[1])\n",
        "\n",
        "# Executar map/reduce\n",
        "resultado_rdd = neg_df.rdd.map(map2).reduceByKey(reduceByKey2).collect()\n",
        "\n",
        "# Extrair resultado do sentimento 'neg'\n",
        "total_en_rdd, total_pt_rdd = [v for k, v in resultado_rdd if k == 'neg'][0]\n",
        "\n",
        "# Validar soma\n",
        "if (total_en_rdd == soma_en) and (total_pt_rdd == soma_pt):\n",
        "    validacao = \"âœ… VALIDAÃ‡ÃƒO BEM-SUCEDIDA! Contagens coincidem.\"\n",
        "else:\n",
        "    validacao = \"âš ï¸ ATENÃ‡ÃƒO! DiferenÃ§a entre mÃ©todos detectada.\"\n",
        "\n",
        "# ImpressÃ£o do resultado final com visual impactante\n",
        "print(f\"\"\"\n",
        "ğŸ“¦ Dados processados com sucesso!\n",
        "\n",
        "ğŸ” Tipo de anÃ¡lise: DiferenÃ§a no total de palavras em textos negativos\n",
        "ğŸ§® Total palavras (InglÃªs) â†’ {soma_en:,}\n",
        "ğŸ§® Total palavras (PortuguÃªs) â†’ {soma_pt:,}\n",
        "ğŸ§® DiferenÃ§a (PT - EN) â†’ {diferenca:,}\n",
        "\n",
        "ğŸ“ˆ ConclusÃ£o: {validacao}\n",
        "\n",
        "ğŸ“ InformaÃ§Ãµes do participante:\n",
        "   - Nome: {nome}\n",
        "   - Registro UniversitÃ¡rio (RU): {ru}\n",
        "\n",
        "ğŸ“Š Resultado Final:\n",
        "   Os textos negativos em portuguÃªs possuem **{diferenca:,}** palavras a mais do que os textos em inglÃªs.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btak7ta4dkZs"
      },
      "source": [
        "## Cria funÃ§Ãµes de REDUCE:\n",
        "\n",
        "- Criar funÃ§Ã£o de reduce para somar o numero de palavras de cada texto portuguÃªs e inglÃªs por \"sentiment\" (dependerÃ¡ de como vocÃª optou por fazer sua funÃ§Ã£o map2).\n",
        "\n",
        "A funÃ§Ã£o reduce irÃ¡ somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HghmU7cQdkZs"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "  # Coloque aqui o seu cÃ³digo para retornar o resultado necessÃ¡rio.\n",
        "  # x e y sÃ£o valores que podem ser ou a tupla vinda da saÃ­da da funÃ§Ã£o map\n",
        "  # contendo quantidade de palavras em inglÃªs e portuguÃªs, ou a diferenÃ§a, a\n",
        "  # depender da sua implementaÃ§Ã£o da funÃ§Ã£o map2.\n",
        "  # Apague a linha abaixo para iniciar seu cÃ³digo.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPV924cndkZs"
      },
      "source": [
        "## AplicaÃ§Ã£o do map/reduce e visualizaÃ§Ã£o do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtraÃ§Ã£o.\n",
        "3. Realizar a subtraÃ§Ã£o das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dsijpn2dkZs"
      },
      "outputs": [],
      "source": [
        "# Linha de cÃ³digo para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.rdd.map(map2).reduceByKey(reduceByKey2).collect()\n",
        "# Coloque aqui suas linhas de cÃ³digo final para imprimir o resultado.\n",
        "# NÃ£o esqueÃ§a seu RU:\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}