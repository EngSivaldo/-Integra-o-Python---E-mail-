{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngSivaldo/-Integra-o-Python---E-mail-/blob/main/trabalho_big_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raa8pHuHdkZj"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "Este notebook foi projetado para guiar os alunos na realização das práticas de Big Data utilizando PySpark. Certifique-se de seguir cada etapa cuidadosamente para garantir a correta execução das atividades.\n",
        "\n",
        "Seu trabalho começará na célula 5. Execute as 4 primeiras células para iniciar a atividade.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcvGRW8kdkZl"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab\n",
        "\n",
        "Aqui, você fará o download do dataset necessário para as atividades. Certifique-se de que o arquivo foi descompactado corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "e54FoMBzdkZm",
        "outputId": "ceedae82-00ec-4e16-e693-1adda70cb665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-18 23:09:59--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "\rimdb-reviews-pt-br.   0%[                    ]       0  --.-KB/s               \rimdb-reviews-pt-br. 100%[===================>]  47.25M   246MB/s    in 0.2s    \n",
            "\n",
            "2025-05-18 23:10:00 (246 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "replace imdb-reviews-pt-br.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-qJVr2dkZn"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab\n",
        "\n",
        "Esta etapa garante que todas as bibliotecas necessárias para o PySpark sejam instaladas no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COjZ7agkdkZn"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN4_Wh_DdkZo"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession\n",
        "\n",
        "A SparkSession é o ponto de entrada para usar o PySpark. Certifique-se de configurar corretamente o nome do aplicativo e o master."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_jQ0zaPzdkZo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atPpgw2vdkZp"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark\n",
        "\n",
        "Não altere este código e use o dataframe imdb_df criado aqui em todo o seu trabalho. A criação de um dataframe diferente deste poderá causar erros na coluna sentiment e isso refletirá em erros de resposta das questões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-nr5sWPPdkZp"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeqjIczDdkZp"
      },
      "source": [
        "# Questão 1\n",
        "\n",
        "Nesta questão, você irá calcular a soma dos IDs para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhC3ICjcdkZq"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "A0tFLvK9dkZq",
        "outputId": "ae86e81c-4316-46a0-b567-212a25da5d5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 INICIANDO ANÁLISE DA ESTRUTURA DO DATAFRAME...\n",
            "\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "🔍 COLUNAS DISPONÍVEIS E TIPOS DE DADOS NO DATAFRAME:\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- text_en: string (nullable = true)\n",
            " |-- text_pt: string (nullable = true)\n",
            " |-- sentiment: string (nullable = true)\n",
            "\n",
            "\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "📌 INTERPRETAÇÃO:\n",
            "  • 'id'         → identificador do registro (string)\n",
            "  • 'text_en'    → texto da review em inglês\n",
            "  • 'text_pt'    → texto da review em português\n",
            "  • 'sentiment'  → sentimento da review (pos ou neg)\n",
            "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
            "\n",
            "📦 Dados processados com sucesso!\n",
            "\n",
            "🔎 Tipo de análise: Soma dos IDs para sentimentos negativos\n",
            "🧮 Método SQL → Resultado: 459568555\n",
            "🧮 Método RDD (map/reduce) → Resultado: 459568555\n",
            "\n",
            "📈 Conclusão: Ambos os métodos retornaram o mesmo valor.\n",
            "✅ A soma foi validada com sucesso!\n",
            "\n",
            "📍 Informações do participante:\n",
            "   - Nome: Sivaldo\n",
            "   - Registro Universitário (RU): 4139872\n",
            "\n",
            "📊 Resultado Final:\n",
            "   A soma total dos IDs associados ao sentimento 'neg' é **459,568,555**.\n"
          ]
        }
      ],
      "source": [
        "# 📌 Informações do aluno\n",
        "ru = \"4139872\"\n",
        "nome = \"Sivaldo\"\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# 📦 INICIANDO ANÁLISE DA ESTRUTURA DO DATAFRAME...\n",
        "print(\"\\n📦 INICIANDO ANÁLISE DA ESTRUTURA DO DATAFRAME...\\n\")\n",
        "print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "print(\"🔍 COLUNAS DISPONÍVEIS E TIPOS DE DADOS NO DATAFRAME:\")\n",
        "print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
        "\n",
        "# Exibir o schema do DataFrame\n",
        "imdb_df.printSchema()\n",
        "\n",
        "print(\"\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "print(\"📌 INTERPRETAÇÃO:\")\n",
        "print(\"  • 'id'         → identificador do registro (string)\")\n",
        "print(\"  • 'text_en'    → texto da review em inglês\")\n",
        "print(\"  • 'text_pt'    → texto da review em português\")\n",
        "print(\"  • 'sentiment'  → sentimento da review (pos ou neg)\")\n",
        "print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
        "\n",
        "# 📌 Removendo registros com 'id' nulo ou vazio\n",
        "df_valid = imdb_df.filter((col(\"id\").isNotNull()) & (col(\"id\") != \"\"))\n",
        "\n",
        "# 🔁 Convertendo coluna 'id' para inteiro\n",
        "df_valid = df_valid.withColumn(\"id_int\", col(\"id\").cast(IntegerType()))\n",
        "\n",
        "# 🔍 Filtrando apenas sentimentos negativos com IDs válidos\n",
        "df_neg = df_valid.filter((col(\"sentiment\") == \"neg\") & col(\"id_int\").isNotNull())\n",
        "\n",
        "# ✅ Soma com Spark SQL (forma segura)\n",
        "soma_sql = df_neg.agg({\"id_int\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "# Função MAP para o RDD\n",
        "def map1(row):\n",
        "    try:\n",
        "        return (row[\"sentiment\"], int(row[\"id\"]))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Função REDUCE para o RDD\n",
        "def reduceByKey1(x, y):\n",
        "    return x + y\n",
        "\n",
        "# Aplicando map/reduce com RDD\n",
        "resultado_rdd = imdb_df.rdd.map(map1).filter(lambda x: x is not None and x[0] == \"neg\").reduceByKey(reduceByKey1).collect()\n",
        "soma_rdd = [v for k, v in resultado_rdd if k == \"neg\"][0]\n",
        "\n",
        "# 🎯 Exibição final com layout profissional\n",
        "print(\"📦 Dados processados com sucesso!\\n\")\n",
        "print(\"🔎 Tipo de análise: Soma dos IDs para sentimentos negativos\")\n",
        "print(f\"🧮 Método SQL → Resultado: {soma_sql}\")\n",
        "print(f\"🧮 Método RDD (map/reduce) → Resultado: {soma_rdd}\")\n",
        "\n",
        "if soma_sql == soma_rdd:\n",
        "    print(\"\\n📈 Conclusão: Ambos os métodos retornaram o mesmo valor.\")\n",
        "    print(\"✅ A soma foi validada com sucesso!\\n\")\n",
        "else:\n",
        "    print(\"\\n⚠️ ATENÇÃO: As somas não coincidem. Verifique os dados!\")\n",
        "\n",
        "print(\"📍 Informações do participante:\")\n",
        "print(f\"   - Nome: {nome}\")\n",
        "print(f\"   - Registro Universitário (RU): {ru}\")\n",
        "\n",
        "print(\"\\n📊 Resultado Final:\")\n",
        "print(f\"   A soma total dos IDs associados ao sentimento 'neg' é **{soma_sql:,.0f}**.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOCzLDcndkZq"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A função reduce irá somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKZuQd2IdkZq"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que serão somados, pois o reduceByKey receberá\n",
        "  # apenas o segundo elemento da tupla vinda da saída da função map.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5_3nbKRdkZr"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "Aqui, você aplicará as funções de map e reduce ao dataframe Spark para calcular os resultados. Não se esqueça de usar o método `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BanrXF13dkZr"
      },
      "outputs": [],
      "source": [
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.rdd.map(map1).reduceByKey(reduceByKey1).collect()\n",
        "# Coloque aqui o código para imprimir o resultado. Não esqueça seu RU:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHJT9-AzdkZr"
      },
      "source": [
        "# Questão 2:\n",
        "\n",
        "Nesta questão, você irá calcular a diferença no número total de palavras entre textos negativos em português e inglês.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "- Subtrair o total de palavras em inglês do total em português."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFO4r3PSdkZr"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para então descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o método .split() e não .split(\" \") de string para separar as palavras em uma lista ou use a função split(coluna de texto, regex) do pyspark com o regex igual à \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "eLzfbFGBdkZs",
        "outputId": "c48a2d98-092b-4dcb-9cc9-533429937248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Iniciando processamento dos dados...\n",
            "🔍 Contando palavras nos textos negativos (inglês e português)...\n",
            "🔁 Aplicando map/reduce para validação...\n",
            "\n",
            "📦 Dados processados com sucesso!\n",
            "\n",
            "🔎 Tipo de análise: Diferença no total de palavras em textos negativos\n",
            "🧮 Total palavras (Inglês) → 5,400,297\n",
            "🧮 Total palavras (Português) → 5,455,273\n",
            "🧮 Diferença (PT - EN) → 54,976\n",
            "\n",
            "📈 Conclusão: ✅ VALIDAÇÃO BEM-SUCEDIDA! Contagens coincidem.\n",
            "\n",
            "📍 Informações do participante:\n",
            "   - Nome: Sivaldo\n",
            "   - Registro Universitário (RU): 4139872\n",
            "\n",
            "📊 Resultado Final:\n",
            "   Os textos negativos em português possuem **54,976** palavras a mais do que os textos em inglês.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importações necessárias\n",
        "from pyspark.sql.functions import col, split, size\n",
        "\n",
        "# Dados do participante\n",
        "ru = \"4139872\"\n",
        "nome = \"Sivaldo\"\n",
        "\n",
        "print(\"📦 Iniciando processamento dos dados...\")\n",
        "\n",
        "# Filtrar registros com sentimento negativo\n",
        "neg_df = imdb_df.filter(col(\"sentiment\") == \"neg\")\n",
        "\n",
        "print(\"🔍 Contando palavras nos textos negativos (inglês e português)...\")\n",
        "\n",
        "# Contar palavras na coluna 'text_en'\n",
        "neg_df = neg_df.withColumn(\"count_en\", size(split(col(\"text_en\"), r\"\\s+\")))\n",
        "\n",
        "# Contar palavras na coluna 'text_pt'\n",
        "neg_df = neg_df.withColumn(\"count_pt\", size(split(col(\"text_pt\"), r\"\\s+\")))\n",
        "\n",
        "# Somar o total de palavras por idioma\n",
        "soma_en = neg_df.agg({\"count_en\": \"sum\"}).collect()[0][0]\n",
        "soma_pt = neg_df.agg({\"count_pt\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "# Calcular diferença total (pt - en)\n",
        "diferenca = soma_pt - soma_en\n",
        "\n",
        "print(\"🔁 Aplicando map/reduce para validação...\")\n",
        "\n",
        "# Função map para mapear sentimento e contagem de palavras\n",
        "def map2(row):\n",
        "    return (row['sentiment'], (row['count_en'], row['count_pt']))\n",
        "\n",
        "# Função reduce para somar palavras por idioma\n",
        "def reduceByKey2(x, y):\n",
        "    return (x[0] + y[0], x[1] + y[1])\n",
        "\n",
        "# Executar map/reduce\n",
        "resultado_rdd = neg_df.rdd.map(map2).reduceByKey(reduceByKey2).collect()\n",
        "\n",
        "# Extrair resultado do sentimento 'neg'\n",
        "total_en_rdd, total_pt_rdd = [v for k, v in resultado_rdd if k == 'neg'][0]\n",
        "\n",
        "# Validar soma\n",
        "if (total_en_rdd == soma_en) and (total_pt_rdd == soma_pt):\n",
        "    validacao = \"✅ VALIDAÇÃO BEM-SUCEDIDA! Contagens coincidem.\"\n",
        "else:\n",
        "    validacao = \"⚠️ ATENÇÃO! Diferença entre métodos detectada.\"\n",
        "\n",
        "# Impressão do resultado final com visual impactante\n",
        "print(f\"\"\"\n",
        "📦 Dados processados com sucesso!\n",
        "\n",
        "🔎 Tipo de análise: Diferença no total de palavras em textos negativos\n",
        "🧮 Total palavras (Inglês) → {soma_en:,}\n",
        "🧮 Total palavras (Português) → {soma_pt:,}\n",
        "🧮 Diferença (PT - EN) → {diferenca:,}\n",
        "\n",
        "📈 Conclusão: {validacao}\n",
        "\n",
        "📍 Informações do participante:\n",
        "   - Nome: {nome}\n",
        "   - Registro Universitário (RU): {ru}\n",
        "\n",
        "📊 Resultado Final:\n",
        "   Os textos negativos em português possuem **{diferenca:,}** palavras a mais do que os textos em inglês.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btak7ta4dkZs"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\" (dependerá de como você optou por fazer sua função map2).\n",
        "\n",
        "A função reduce irá somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HghmU7cQdkZs"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que podem ser ou a tupla vinda da saída da função map\n",
        "  # contendo quantidade de palavras em inglês e português, ou a diferença, a\n",
        "  # depender da sua implementação da função map2.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPV924cndkZs"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dsijpn2dkZs"
      },
      "outputs": [],
      "source": [
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.rdd.map(map2).reduceByKey(reduceByKey2).collect()\n",
        "# Coloque aqui suas linhas de código final para imprimir o resultado.\n",
        "# Não esqueça seu RU:\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}